{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twPjzixbmzA9"
      },
      "outputs": [],
      "source": [
        "from pandas import read_csv\n",
        "import pandas as pd\n",
        "\n",
        "# Specify the encoding as detected ('ascii')\n",
        "train_data = pd.read_csv('/content/srsstat_train_data.csv', encoding='ascii')  # Updated encoding to 'ascii'\n",
        "\n",
        "test_data = pd.read_csv('/content/srsstat_test_data.csv', encoding='ascii')  # Updated encoding to 'ascii'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.value_counts('cluster')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "pHSr-N5-m26T",
        "outputId": "6a796777-2fa6-4659-c8e2-a3e7ad88cde1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "cluster\n",
              "1    7202\n",
              "0    6913\n",
              "2    6885\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cluster</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7202</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6885</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85WaT6k0m4VD",
        "outputId": "62e4cbf7-c3f4-45e6-a747-1b53b6ceb398"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 21000 entries, 0 to 20999\n",
            "Data columns (total 87 columns):\n",
            " #   Column                     Non-Null Count  Dtype  \n",
            "---  ------                     --------------  -----  \n",
            " 0   customer_id                21000 non-null  int64  \n",
            " 1   age                        21000 non-null  int64  \n",
            " 2   gender                     21000 non-null  object \n",
            " 3   income_bracket             21000 non-null  object \n",
            " 4   loyalty_program            21000 non-null  object \n",
            " 5   membership_years           21000 non-null  int64  \n",
            " 6   churned                    21000 non-null  object \n",
            " 7   marital_status             21000 non-null  object \n",
            " 8   number_of_children         21000 non-null  int64  \n",
            " 9   education_level            21000 non-null  object \n",
            " 10  occupation                 21000 non-null  object \n",
            " 11  transaction_id             21000 non-null  int64  \n",
            " 12  product_id                 21000 non-null  int64  \n",
            " 13  product_category           21000 non-null  object \n",
            " 14  quantity                   21000 non-null  int64  \n",
            " 15  unit_price                 21000 non-null  float64\n",
            " 16  discount_applied           21000 non-null  float64\n",
            " 17  payment_method             21000 non-null  object \n",
            " 18  store_location             21000 non-null  object \n",
            " 19  transaction_hour           21000 non-null  int64  \n",
            " 20  day_of_week                21000 non-null  object \n",
            " 21  week_of_year               21000 non-null  int64  \n",
            " 22  month_of_year              21000 non-null  int64  \n",
            " 23  avg_purchase_value         21000 non-null  float64\n",
            " 24  purchase_frequency         21000 non-null  object \n",
            " 25  avg_discount_used          21000 non-null  float64\n",
            " 26  preferred_store            21000 non-null  object \n",
            " 27  online_purchases           21000 non-null  int64  \n",
            " 28  in_store_purchases         21000 non-null  int64  \n",
            " 29  avg_items_per_transaction  21000 non-null  float64\n",
            " 30  avg_transaction_value      21000 non-null  float64\n",
            " 31  total_returned_items       21000 non-null  int64  \n",
            " 32  total_returned_value       21000 non-null  float64\n",
            " 33  total_sales_over_lastyear  21000 non-null  float64\n",
            " 34  total_transactions         21000 non-null  int64  \n",
            " 35  total_items_purchased      21000 non-null  int64  \n",
            " 36  total_discounts_received   21000 non-null  float64\n",
            " 37  avg_spent_per_category     21000 non-null  float64\n",
            " 38  max_single_purchase_value  21000 non-null  float64\n",
            " 39  min_single_purchase_value  21000 non-null  float64\n",
            " 40  product_name               21000 non-null  object \n",
            " 41  product_brand              21000 non-null  object \n",
            " 42  product_rating             21000 non-null  float64\n",
            " 43  product_review_count       21000 non-null  int64  \n",
            " 44  product_stock              21000 non-null  int64  \n",
            " 45  product_return_rate        21000 non-null  float64\n",
            " 46  product_size               21000 non-null  object \n",
            " 47  product_weight             21000 non-null  float64\n",
            " 48  product_color              21000 non-null  object \n",
            " 49  product_material           21000 non-null  object \n",
            " 50  product_shelf_life         21000 non-null  int64  \n",
            " 51  promotion_id               21000 non-null  int64  \n",
            " 52  promotion_type             21000 non-null  object \n",
            " 53  promotion_effectiveness    21000 non-null  object \n",
            " 54  promotion_channel          21000 non-null  object \n",
            " 55  promotion_target_audience  21000 non-null  object \n",
            " 56  customer_zip_code          21000 non-null  int64  \n",
            " 57  customer_city              21000 non-null  object \n",
            " 58  customer_state             21000 non-null  object \n",
            " 59  store_zip_code             21000 non-null  int64  \n",
            " 60  store_city                 21000 non-null  object \n",
            " 61  store_state                21000 non-null  object \n",
            " 62  distance_to_store          21000 non-null  float64\n",
            " 63  holiday_season             21000 non-null  object \n",
            " 64  season                     21000 non-null  object \n",
            " 65  weekend                    21000 non-null  object \n",
            " 66  customer_support_calls     21000 non-null  int64  \n",
            " 67  email_subscriptions        21000 non-null  object \n",
            " 68  app_usage                  21000 non-null  object \n",
            " 69  website_visits             21000 non-null  int64  \n",
            " 70  social_media_engagement    21000 non-null  object \n",
            " 71  days_since_last_purchase   21000 non-null  int64  \n",
            " 72  high_value_purchase        21000 non-null  bool   \n",
            " 73  high_value_quantity        21000 non-null  bool   \n",
            " 74  transaction_month          21000 non-null  int64  \n",
            " 75  transaction_year           21000 non-null  int64  \n",
            " 76  product_expiry_date_month  21000 non-null  int64  \n",
            " 77  product_expiry_date_year   21000 non-null  int64  \n",
            " 78  product_manufacture_month  21000 non-null  int64  \n",
            " 79  product_manufacture_year   21000 non-null  int64  \n",
            " 80  promotion_end_month        21000 non-null  int64  \n",
            " 81  promotion_end_year         21000 non-null  int64  \n",
            " 82  last_purchase_month        21000 non-null  int64  \n",
            " 83  last_purchase_year         21000 non-null  int64  \n",
            " 84  promotion_start_month      21000 non-null  int64  \n",
            " 85  promotion_start_year       21000 non-null  int64  \n",
            " 86  cluster                    21000 non-null  int64  \n",
            "dtypes: bool(2), float64(16), int64(37), object(32)\n",
            "memory usage: 13.7+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KNN Without Balancing - All Features"
      ],
      "metadata": {
        "id": "OlfL8Ly6m72j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, recall_score, precision_score, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from collections import Counter\n",
        "from scipy.stats import randint\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Verify train_data and test_data exist from previous steps\n",
        "if 'train_data' not in globals() or 'test_data' not in globals():\n",
        "    raise ValueError(\"train_data or test_data not found! Ensure previous steps are executed.\")\n",
        "\n",
        "# Define the attributes as specified\n",
        "selected_features = [\n",
        "    # Categorical attributes\n",
        "    'last_purchase_month', 'promotion_end_month', 'product_manufacture_month',\n",
        "    'month_of_year', 'product_expiry_date_year', 'product_manufacture_year',\n",
        "    'transaction_year', 'product_expiry_date_month', 'transaction_month',\n",
        "    'high_value_purchase', 'week_of_year', 'promotion_start_month', 'day_of_week',\n",
        "    'purchase_frequency', 'customer_city', 'gender', 'weekend', 'store_state',\n",
        "    'email_subscriptions', 'store_location', 'high_value_quantity',\n",
        "    # Numeric attributes\n",
        "    'customer_support_calls', 'product_review_count', 'days_since_last_purchase',\n",
        "    'online_purchases', 'distance_to_store', 'product_rating', 'total_transactions',\n",
        "    'product_weight', 'total_items_purchased', 'unit_price', 'total_returned_value',\n",
        "    'membership_years', 'discount_applied', 'avg_discount_used', 'product_shelf_life',\n",
        "    'total_returned_items', 'transaction_hour', 'min_single_purchase_value',\n",
        "    'number_of_children', 'product_stock', 'avg_purchase_value',\n",
        "    'avg_items_per_transaction', 'website_visits', 'age', 'max_single_purchase_value',\n",
        "    'avg_spent_per_category', 'total_discounts_received', 'product_return_rate',\n",
        "    'avg_transaction_value', 'in_store_purchases'\n",
        "]\n",
        "\n",
        "# Filter features that exist in train_data\n",
        "selected_features = [col for col in selected_features if col in train_data.columns]\n",
        "print(\"Selected features for KNN:\", selected_features)\n",
        "\n",
        "# Define features (X) and target (y) for train and test sets\n",
        "X_train = train_data[selected_features]\n",
        "y_train = train_data['cluster']\n",
        "X_test = test_data[selected_features]\n",
        "y_test = test_data['cluster']\n",
        "\n",
        "# Print class distribution in training set\n",
        "print(\"Class Distribution in Training Set:\", Counter(y_train))\n",
        "\n",
        "# Identify categorical and numeric columns among selected features\n",
        "categorical_cols = [col for col in selected_features if col in ['last_purchase_month', 'promotion_end_month', 'product_manufacture_month', 'month_of_year', 'product_expiry_date_year',\n",
        "                                                                'product_manufacture_year', 'transaction_year', 'product_expiry_date_month', 'transaction_month', 'high_value_purchase',\n",
        "                                                                'week_of_year', 'promotion_start_month', 'day_of_week', 'purchase_frequency', 'customer_city', 'gender', 'weekend', 'store_state',\n",
        "                                                                'email_subscriptions', 'store_location', 'high_value_quantity']]\n",
        "numeric_cols = [col for col in selected_features if col not in categorical_cols]\n",
        "\n",
        "# Preprocessing pipeline for encoding and scaling\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='mean')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ]), numeric_cols),\n",
        "    ('cat', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False))\n",
        "    ]), categorical_cols)\n",
        "])\n",
        "\n",
        "# Create pipeline with preprocessing and KNN\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', KNeighborsClassifier())\n",
        "])\n",
        "\n",
        "# Define parameter distribution for RandomizedSearchCV\n",
        "param_dist = {\n",
        "    'classifier__n_neighbors': randint(3, 20),\n",
        "    'classifier__weights': ['uniform', 'distance'],\n",
        "    'classifier__metric': ['euclidean', 'manhattan', 'minkowski']\n",
        "}\n",
        "\n",
        "# Perform RandomizedSearchCV for hyperparameter tuning\n",
        "random_search = RandomizedSearchCV(pipeline, param_distributions=param_dist, n_iter=10, cv=5,\n",
        "                                   scoring='accuracy', n_jobs=-1, random_state=42)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Best hyperparameters\n",
        "best_params = random_search.best_params_\n",
        "print(f\"Best Hyperparameters: {best_params}\")\n",
        "\n",
        "# Train KNN with the best parameters\n",
        "best_model = random_search.best_estimator_\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions on test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Get unique cluster labels for classification report\n",
        "unique_clusters = sorted(y_train.unique())\n",
        "target_names = [f'Cluster {i}' for i in unique_clusters]\n",
        "\n",
        "# Classification Report and Metrics\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=target_names))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# Accuracy, Recall, Precision\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Calculate Specificity for each cluster\n",
        "def calculate_specificity(class_idx):\n",
        "    tn = cm.sum() - cm[:, class_idx].sum() - cm[class_idx, :].sum() + cm[class_idx, class_idx]\n",
        "    fp = cm[:, class_idx].sum() - cm[class_idx, class_idx]\n",
        "    return tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "\n",
        "# Calculate specificity for each cluster\n",
        "specificity_scores = {f'Cluster {i}': calculate_specificity(idx) for idx, i in enumerate(unique_clusters)}\n",
        "\n",
        "# Print metrics\n",
        "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "for cluster, spec in specificity_scores.items():\n",
        "    print(f\"Specificity for {cluster}: {spec:.4f}\")\n",
        "\n",
        "print(\"-----------------------------------------------\")\n",
        "\n",
        "# --- Cross-Validation on Full Dataset ---\n",
        "# Combine train and test data for cross-validation\n",
        "X_full = pd.concat([X_train, X_test], axis=0)\n",
        "y_full = pd.concat([y_train, y_test], axis=0)\n",
        "\n",
        "# Perform 5-fold cross-validation using the best model\n",
        "cv_scores = cross_val_score(best_model, X_full, y_full, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "print(f\"\\nCross-Validation Results (5-fold):\")\n",
        "print(f\"Mean Accuracy: {cv_scores.mean():.4f}\")\n",
        "print(f\"Standard Deviation: {cv_scores.std():.4f}\")\n",
        "print(f\"Individual Fold Scores: {cv_scores}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9MORLYIm5m7",
        "outputId": "091b6ce1-1b75-4e73-e8f7-8bb0af138e8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected features for KNN: ['last_purchase_month', 'promotion_end_month', 'product_manufacture_month', 'month_of_year', 'product_expiry_date_year', 'product_manufacture_year', 'transaction_year', 'product_expiry_date_month', 'transaction_month', 'high_value_purchase', 'week_of_year', 'promotion_start_month', 'day_of_week', 'purchase_frequency', 'customer_city', 'gender', 'weekend', 'store_state', 'email_subscriptions', 'store_location', 'high_value_quantity', 'customer_support_calls', 'product_review_count', 'days_since_last_purchase', 'online_purchases', 'distance_to_store', 'product_rating', 'total_transactions', 'product_weight', 'total_items_purchased', 'unit_price', 'total_returned_value', 'membership_years', 'discount_applied', 'avg_discount_used', 'product_shelf_life', 'total_returned_items', 'transaction_hour', 'min_single_purchase_value', 'number_of_children', 'product_stock', 'avg_purchase_value', 'avg_items_per_transaction', 'website_visits', 'age', 'max_single_purchase_value', 'avg_spent_per_category', 'total_discounts_received', 'product_return_rate', 'avg_transaction_value', 'in_store_purchases']\n",
            "Class Distribution in Training Set: Counter({1: 7202, 0: 6913, 2: 6885})\n",
            "Best Hyperparameters: {'classifier__metric': 'minkowski', 'classifier__n_neighbors': 17, 'classifier__weights': 'distance'}\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Cluster 0       0.65      0.67      0.66      3019\n",
            "   Cluster 1       0.64      0.64      0.64      3055\n",
            "   Cluster 2       0.66      0.64      0.65      2926\n",
            "\n",
            "    accuracy                           0.65      9000\n",
            "   macro avg       0.65      0.65      0.65      9000\n",
            "weighted avg       0.65      0.65      0.65      9000\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2033  550  436]\n",
            " [ 585 1960  510]\n",
            " [ 511  545 1870]]\n",
            "\n",
            "Accuracy: 0.6514\n",
            "Recall: 0.6514\n",
            "Precision: 0.6516\n",
            "Specificity for Cluster 0: 0.8168\n",
            "Specificity for Cluster 1: 0.8158\n",
            "Specificity for Cluster 2: 0.8443\n",
            "-----------------------------------------------\n",
            "\n",
            "Cross-Validation Results (5-fold):\n",
            "Mean Accuracy: 0.6535\n",
            "Standard Deviation: 0.0039\n",
            "Individual Fold Scores: [0.66066667 0.65116667 0.654      0.65283333 0.649     ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KNN - Without Balancing - Selected Features"
      ],
      "metadata": {
        "id": "HfoYTdDUnEkt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, recall_score, precision_score, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from collections import Counter\n",
        "from scipy.stats import randint\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Verify train_data and test_data exist from previous steps\n",
        "if 'train_data' not in globals() or 'test_data' not in globals():\n",
        "    raise ValueError(\"train_data or test_data not found! Ensure previous steps are executed.\")\n",
        "\n",
        "# Define the attributes as specified\n",
        "selected_features = [\n",
        "    # Categorical attributes\n",
        "    'last_purchase_month', 'promotion_end_month', 'product_manufacture_month',\n",
        "    'month_of_year', 'product_expiry_date_year', 'product_manufacture_year',\n",
        "    'transaction_year', 'product_expiry_date_month', 'transaction_month',\n",
        "    'high_value_purchase', 'week_of_year', 'promotion_start_month', 'day_of_week',\n",
        "    'purchase_frequency', 'customer_city', 'gender', 'weekend', 'store_state',\n",
        "    'email_subscriptions', 'store_location', 'high_value_quantity',\n",
        "    # Numeric attributes\n",
        "    'customer_support_calls', 'product_review_count', 'days_since_last_purchase',\n",
        "    'online_purchases', 'distance_to_store', 'product_rating', 'total_transactions',\n",
        "    'product_weight', 'total_items_purchased', 'unit_price', 'total_returned_value',\n",
        "    'membership_years', 'discount_applied', 'avg_discount_used', 'product_shelf_life',\n",
        "    'total_returned_items', 'transaction_hour', 'min_single_purchase_value',\n",
        "    'number_of_children', 'product_stock', 'avg_purchase_value',\n",
        "    'avg_items_per_transaction', 'website_visits', 'age', 'max_single_purchase_value',\n",
        "    'avg_spent_per_category', 'total_discounts_received', 'product_return_rate',\n",
        "    'avg_transaction_value', 'in_store_purchases'\n",
        "]\n",
        "\n",
        "# Filter features that exist in train_data\n",
        "selected_features = [col for col in selected_features if col in train_data.columns]\n",
        "print(\"Initial selected features:\", selected_features)\n",
        "\n",
        "# Define features (X) and target (y) for train and test sets\n",
        "X_train = train_data[selected_features]\n",
        "y_train = train_data['cluster']\n",
        "X_test = test_data[selected_features]\n",
        "y_test = test_data['cluster']\n",
        "\n",
        "# Print class distribution in training set\n",
        "print(\"Class Distribution in Training Set:\", Counter(y_train))\n",
        "\n",
        "# Identify categorical and numeric columns among selected features\n",
        "categorical_cols = [col for col in selected_features if col in ['last_purchase_month', 'promotion_end_month', 'product_manufacture_month', 'month_of_year', 'product_expiry_date_year',\n",
        "                                                                'product_manufacture_year', 'transaction_year', 'product_expiry_date_month', 'transaction_month', 'high_value_purchase',\n",
        "                                                                'week_of_year', 'promotion_start_month', 'day_of_week', 'purchase_frequency', 'customer_city', 'gender', 'weekend', 'store_state',\n",
        "                                                                'email_subscriptions', 'store_location', 'high_value_quantity']]\n",
        "numeric_cols = [col for col in selected_features if col not in categorical_cols]\n",
        "\n",
        "# Preprocessing pipeline for encoding and scaling\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='mean')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ]), numeric_cols),\n",
        "    ('cat', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False))\n",
        "    ]), categorical_cols)\n",
        "])\n",
        "\n",
        "# Preprocess the data to get transformed features\n",
        "X_train_transformed = preprocessor.fit_transform(X_train)\n",
        "X_test_transformed = preprocessor.transform(X_test)\n",
        "\n",
        "# Get transformed feature names\n",
        "feature_names = preprocessor.get_feature_names_out()\n",
        "\n",
        "# Convert transformed data to DataFrame for feature selection\n",
        "X_train_transformed_df = pd.DataFrame(X_train_transformed, columns=feature_names)\n",
        "X_test_transformed_df = pd.DataFrame(X_test_transformed, columns=feature_names)\n",
        "\n",
        "# Feature selection using Random Forest to get feature importance\n",
        "base_model = RandomForestClassifier(n_estimators=50, random_state=42)\n",
        "base_model.fit(X_train_transformed_df, y_train)\n",
        "feature_importances = base_model.feature_importances_\n",
        "top_features_idx = np.argsort(feature_importances)[-10:]  # Get the top 10 features\n",
        "top_10_features = feature_names[top_features_idx]\n",
        "print(f\"Top 10 Features: {list(top_10_features)}\")\n",
        "\n",
        "# Subset data with selected features\n",
        "X_train_selected = X_train_transformed_df[top_10_features]\n",
        "X_test_selected = X_test_transformed_df[top_10_features]\n",
        "\n",
        "# Create pipeline with KNN (no preprocessing needed since data is pre-transformed)\n",
        "pipeline = Pipeline([\n",
        "    ('classifier', KNeighborsClassifier())\n",
        "])\n",
        "\n",
        "# Define parameter distribution for RandomizedSearchCV\n",
        "param_dist = {\n",
        "    'classifier__n_neighbors': randint(3, 20),\n",
        "    'classifier__weights': ['uniform', 'distance'],\n",
        "    'classifier__metric': ['euclidean', 'manhattan', 'minkowski']\n",
        "}\n",
        "\n",
        "# Perform RandomizedSearchCV for hyperparameter tuning\n",
        "random_search = RandomizedSearchCV(pipeline, param_distributions=param_dist, n_iter=10, cv=5,\n",
        "                                   scoring='accuracy', n_jobs=-1, random_state=42)\n",
        "random_search.fit(X_train_selected, y_train)\n",
        "\n",
        "# Best hyperparameters\n",
        "best_params = random_search.best_params_\n",
        "print(f\"Best Hyperparameters: {best_params}\")\n",
        "\n",
        "# Train KNN with the best parameters\n",
        "best_model = random_search.best_estimator_\n",
        "best_model.fit(X_train_selected, y_train)\n",
        "\n",
        "# Predictions on test set\n",
        "y_pred = best_model.predict(X_test_selected)\n",
        "\n",
        "# Get unique cluster labels for classification report\n",
        "unique_clusters = sorted(y_train.unique())\n",
        "target_names = [f'Cluster {i}' for i in unique_clusters]\n",
        "\n",
        "# Classification Report and Metrics\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=target_names))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# Accuracy, Recall, Precision\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Calculate Specificity for each cluster\n",
        "def calculate_specificity(class_idx):\n",
        "    tn = cm.sum() - cm[:, class_idx].sum() - cm[class_idx, :].sum() + cm[class_idx, class_idx]\n",
        "    fp = cm[:, class_idx].sum() - cm[class_idx, class_idx]\n",
        "    return tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "\n",
        "# Calculate specificity for each cluster\n",
        "specificity_scores = {f'Cluster {i}': calculate_specificity(idx) for idx, i in enumerate(unique_clusters)}\n",
        "\n",
        "# Print metrics\n",
        "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "for cluster, spec in specificity_scores.items():\n",
        "    print(f\"Specificity for {cluster}: {spec:.4f}\")\n",
        "\n",
        "print(\"-----------------------------------------------\")\n",
        "\n",
        "# --- Cross-Validation on Full Dataset ---\n",
        "# Combine train and test data for cross-validation\n",
        "X_full = pd.concat([X_train_selected, X_test_selected], axis=0)\n",
        "y_full = pd.concat([y_train, y_test], axis=0)\n",
        "\n",
        "# Perform 5-fold cross-validation using the best model\n",
        "cv_scores = cross_val_score(best_model, X_full, y_full, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "print(f\"\\nCross-Validation Results (5-fold):\")\n",
        "print(f\"Mean Accuracy: {cv_scores.mean():.4f}\")\n",
        "print(f\"Standard Deviation: {cv_scores.std():.4f}\")\n",
        "print(f\"Individual Fold Scores: {cv_scores}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3eZlSBcnHL9",
        "outputId": "70b49b79-979f-46ab-934c-781a0ad6ccf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial selected features: ['last_purchase_month', 'promotion_end_month', 'product_manufacture_month', 'month_of_year', 'product_expiry_date_year', 'product_manufacture_year', 'transaction_year', 'product_expiry_date_month', 'transaction_month', 'high_value_purchase', 'week_of_year', 'promotion_start_month', 'day_of_week', 'purchase_frequency', 'customer_city', 'gender', 'weekend', 'store_state', 'email_subscriptions', 'store_location', 'high_value_quantity', 'customer_support_calls', 'product_review_count', 'days_since_last_purchase', 'online_purchases', 'distance_to_store', 'product_rating', 'total_transactions', 'product_weight', 'total_items_purchased', 'unit_price', 'total_returned_value', 'membership_years', 'discount_applied', 'avg_discount_used', 'product_shelf_life', 'total_returned_items', 'transaction_hour', 'min_single_purchase_value', 'number_of_children', 'product_stock', 'avg_purchase_value', 'avg_items_per_transaction', 'website_visits', 'age', 'max_single_purchase_value', 'avg_spent_per_category', 'total_discounts_received', 'product_return_rate', 'avg_transaction_value', 'in_store_purchases']\n",
            "Class Distribution in Training Set: Counter({1: 7202, 0: 6913, 2: 6885})\n",
            "Top 10 Features: ['num__total_transactions', 'num__product_rating', 'num__unit_price', 'num__total_items_purchased', 'num__product_weight', 'num__distance_to_store', 'num__online_purchases', 'num__customer_support_calls', 'num__days_since_last_purchase', 'num__product_review_count']\n",
            "Best Hyperparameters: {'classifier__metric': 'minkowski', 'classifier__n_neighbors': 17, 'classifier__weights': 'uniform'}\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Cluster 0       0.60      0.64      0.62      3019\n",
            "   Cluster 1       0.60      0.60      0.60      3055\n",
            "   Cluster 2       0.59      0.55      0.57      2926\n",
            "\n",
            "    accuracy                           0.60      9000\n",
            "   macro avg       0.60      0.60      0.60      9000\n",
            "weighted avg       0.60      0.60      0.60      9000\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1942  557  520]\n",
            " [ 632 1848  575]\n",
            " [ 645  685 1596]]\n",
            "\n",
            "Accuracy: 0.5984\n",
            "Recall: 0.5984\n",
            "Precision: 0.5982\n",
            "Specificity for Cluster 0: 0.7865\n",
            "Specificity for Cluster 1: 0.7911\n",
            "Specificity for Cluster 2: 0.8197\n",
            "-----------------------------------------------\n",
            "\n",
            "Cross-Validation Results (5-fold):\n",
            "Mean Accuracy: 0.6025\n",
            "Standard Deviation: 0.0046\n",
            "Individual Fold Scores: [0.5995     0.604      0.61066667 0.59766667 0.60066667]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KNN - With Balancing - All Features"
      ],
      "metadata": {
        "id": "7_HQfyFUnPNX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, recall_score, precision_score, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline as SKPipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from collections import Counter\n",
        "from scipy.stats import randint\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Verify train_data and test_data exist from previous steps\n",
        "if 'train_data' not in globals() or 'test_data' not in globals():\n",
        "    raise ValueError(\"train_data or test_data not found! Ensure previous steps are executed.\")\n",
        "\n",
        "# Define the attributes as specified\n",
        "selected_features = [\n",
        "    # Categorical attributes\n",
        "    'last_purchase_month', 'promotion_end_month', 'product_manufacture_month',\n",
        "    'month_of_year', 'product_expiry_date_year', 'product_manufacture_year',\n",
        "    'transaction_year', 'product_expiry_date_month', 'transaction_month',\n",
        "    'high_value_purchase', 'week_of_year', 'promotion_start_month', 'day_of_week',\n",
        "    'purchase_frequency', 'customer_city', 'gender', 'weekend', 'store_state',\n",
        "    'email_subscriptions', 'store_location', 'high_value_quantity',\n",
        "    # Numeric attributes\n",
        "    'customer_support_calls', 'product_review_count', 'days_since_last_purchase',\n",
        "    'online_purchases', 'distance_to_store', 'product_rating', 'total_transactions',\n",
        "    'product_weight', 'total_items_purchased', 'unit_price', 'total_returned_value',\n",
        "    'membership_years', 'discount_applied', 'avg_discount_used', 'product_shelf_life',\n",
        "    'total_returned_items', 'transaction_hour', 'min_single_purchase_value',\n",
        "    'number_of_children', 'product_stock', 'avg_purchase_value',\n",
        "    'avg_items_per_transaction', 'website_visits', 'age', 'max_single_purchase_value',\n",
        "    'avg_spent_per_category', 'total_discounts_received', 'product_return_rate',\n",
        "    'avg_transaction_value', 'in_store_purchases'\n",
        "]\n",
        "\n",
        "# Filter features that exist in train_data\n",
        "selected_features = [col for col in selected_features if col in train_data.columns]\n",
        "print(\"Initial selected features:\", selected_features)\n",
        "\n",
        "# Combine train_data and test_data for balancing\n",
        "data = pd.concat([train_data, test_data], axis=0)\n",
        "X = data[selected_features]\n",
        "y = data['cluster']\n",
        "\n",
        "# Print class distribution before balancing\n",
        "print(\"Class Distribution Before Balancing:\", Counter(y))\n",
        "\n",
        "# Identify categorical and numeric columns among selected features\n",
        "categorical_cols = [col for col in selected_features if col in ['last_purchase_month', 'promotion_end_month', 'product_manufacture_month', 'month_of_year', 'product_expiry_date_year',\n",
        "                                                                'product_manufacture_year', 'transaction_year', 'product_expiry_date_month', 'transaction_month', 'high_value_purchase',\n",
        "                                                                'week_of_year', 'promotion_start_month', 'day_of_week', 'purchase_frequency', 'customer_city', 'gender', 'weekend', 'store_state',\n",
        "                                                                'email_subscriptions', 'store_location', 'high_value_quantity']]\n",
        "numeric_cols = [col for col in selected_features if col not in categorical_cols]\n",
        "\n",
        "# Preprocessing pipeline for encoding and scaling\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='mean')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ]), numeric_cols),\n",
        "    ('cat', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False))\n",
        "    ]), categorical_cols)\n",
        "])\n",
        "\n",
        "# Preprocess the data before balancing to avoid string-to-float issues\n",
        "X_transformed = preprocessor.fit_transform(X)\n",
        "feature_names = preprocessor.get_feature_names_out()\n",
        "\n",
        "# Define balancing strategy: target 2,000 samples per cluster\n",
        "sampling_strategy_under = {1: 10000}\n",
        "sampling_strategy_over = {0: 10000, 2: 10000}\n",
        "\n",
        "# Create a pipeline for undersampling and oversampling\n",
        "balancing_pipeline = ImbPipeline([\n",
        "    ('undersample', RandomUnderSampler(sampling_strategy=sampling_strategy_under, random_state=42)),\n",
        "    ('oversample', SMOTE(sampling_strategy=sampling_strategy_over, random_state=42))\n",
        "])\n",
        "\n",
        "# Apply balancing\n",
        "X_resampled, y_resampled = balancing_pipeline.fit_resample(X_transformed, y)\n",
        "\n",
        "# Check dataset distribution after balancing\n",
        "print(\"After Balancing:\", Counter(y_resampled))\n",
        "\n",
        "# Split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create pipeline with KNN (no preprocessing needed since data is pre-transformed)\n",
        "pipeline = SKPipeline([\n",
        "    ('classifier', KNeighborsClassifier())\n",
        "])\n",
        "\n",
        "# Define parameter distribution for RandomizedSearchCV\n",
        "param_dist = {\n",
        "    'classifier__n_neighbors': randint(3, 20),\n",
        "    'classifier__weights': ['uniform', 'distance'],\n",
        "    'classifier__metric': ['euclidean', 'manhattan', 'minkowski']\n",
        "}\n",
        "\n",
        "# Perform RandomizedSearchCV for hyperparameter tuning\n",
        "random_search = RandomizedSearchCV(pipeline, param_distributions=param_dist, n_iter=10, cv=5,\n",
        "                                   scoring='accuracy', n_jobs=-1, random_state=42)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Best hyperparameters\n",
        "best_params = random_search.best_params_\n",
        "print(f\"Best Hyperparameters: {best_params}\")\n",
        "\n",
        "# Train KNN with the best parameters\n",
        "best_model = random_search.best_estimator_\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions on test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Get unique cluster labels for classification report\n",
        "unique_clusters = sorted(np.unique(y_train))\n",
        "target_names = [f'Cluster {i}' for i in unique_clusters]\n",
        "\n",
        "# Classification Report and Metrics\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=target_names))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# Accuracy, Recall, Precision\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Calculate Specificity for each cluster\n",
        "def calculate_specificity(class_idx):\n",
        "    tn = cm.sum() - cm[:, class_idx].sum() - cm[class_idx, :].sum() + cm[class_idx, class_idx]\n",
        "    fp = cm[:, class_idx].sum() - cm[class_idx, class_idx]\n",
        "    return tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "\n",
        "# Calculate specificity for each cluster\n",
        "specificity_scores = {f'Cluster {i}': calculate_specificity(idx) for idx, i in enumerate(unique_clusters)}\n",
        "\n",
        "# Print metrics\n",
        "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "for cluster, spec in specificity_scores.items():\n",
        "    print(f\"Specificity for {cluster}: {spec:.4f}\")\n",
        "\n",
        "print(\"-----------------------------------------------\")\n",
        "\n",
        "# --- Cross-Validation on Full Dataset ---\n",
        "# Combine train and test data for cross-validation\n",
        "X_full = np.vstack([X_train, X_test])\n",
        "y_full = np.concatenate([y_train, y_test])\n",
        "\n",
        "# Perform 5-fold cross-validation using the best model\n",
        "cv_scores = cross_val_score(best_model, X_full, y_full, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "print(f\"\\nCross-Validation Results (5-fold):\")\n",
        "print(f\"Mean Accuracy: {cv_scores.mean():.4f}\")\n",
        "print(f\"Standard Deviation: {cv_scores.std():.4f}\")\n",
        "print(f\"Individual Fold Scores: {cv_scores}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3DlgMYknRIX",
        "outputId": "eca5c2df-f2be-4d5a-d8e2-0e2906cbd5a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial selected features: ['last_purchase_month', 'promotion_end_month', 'product_manufacture_month', 'month_of_year', 'product_expiry_date_year', 'product_manufacture_year', 'transaction_year', 'product_expiry_date_month', 'transaction_month', 'high_value_purchase', 'week_of_year', 'promotion_start_month', 'day_of_week', 'purchase_frequency', 'customer_city', 'gender', 'weekend', 'store_state', 'email_subscriptions', 'store_location', 'high_value_quantity', 'customer_support_calls', 'product_review_count', 'days_since_last_purchase', 'online_purchases', 'distance_to_store', 'product_rating', 'total_transactions', 'product_weight', 'total_items_purchased', 'unit_price', 'total_returned_value', 'membership_years', 'discount_applied', 'avg_discount_used', 'product_shelf_life', 'total_returned_items', 'transaction_hour', 'min_single_purchase_value', 'number_of_children', 'product_stock', 'avg_purchase_value', 'avg_items_per_transaction', 'website_visits', 'age', 'max_single_purchase_value', 'avg_spent_per_category', 'total_discounts_received', 'product_return_rate', 'avg_transaction_value', 'in_store_purchases']\n",
            "Class Distribution Before Balancing: Counter({1: 10257, 0: 9932, 2: 9811})\n",
            "After Balancing: Counter({0: 10000, 1: 10000, 2: 10000})\n",
            "Best Hyperparameters: {'classifier__metric': 'minkowski', 'classifier__n_neighbors': 17, 'classifier__weights': 'distance'}\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Cluster 0       0.66      0.68      0.67      3007\n",
            "   Cluster 1       0.68      0.60      0.64      2995\n",
            "   Cluster 2       0.65      0.72      0.68      2998\n",
            "\n",
            "    accuracy                           0.66      9000\n",
            "   macro avg       0.66      0.66      0.66      9000\n",
            "weighted avg       0.66      0.66      0.66      9000\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2038  446  523]\n",
            " [ 570 1786  639]\n",
            " [ 468  386 2144]]\n",
            "\n",
            "Accuracy: 0.6631\n",
            "Recall: 0.6631\n",
            "Precision: 0.6644\n",
            "Specificity for Cluster 0: 0.8268\n",
            "Specificity for Cluster 1: 0.8614\n",
            "Specificity for Cluster 2: 0.8064\n",
            "-----------------------------------------------\n",
            "\n",
            "Cross-Validation Results (5-fold):\n",
            "Mean Accuracy: 0.6553\n",
            "Standard Deviation: 0.0056\n",
            "Individual Fold Scores: [0.65583333 0.64866667 0.6495     0.65883333 0.6635    ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KNN - With Balancing - Selected Features"
      ],
      "metadata": {
        "id": "P6Md1MhfndN8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, recall_score, precision_score, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline as SKPipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from collections import Counter\n",
        "from scipy.stats import randint\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Verify train_data and test_data exist from previous steps\n",
        "if 'train_data' not in globals() or 'test_data' not in globals():\n",
        "    raise ValueError(\"train_data or test_data not found! Ensure previous steps are executed.\")\n",
        "\n",
        "# Define the attributes as specified\n",
        "selected_features = [\n",
        "    # Categorical attributes\n",
        "    'last_purchase_month', 'promotion_end_month', 'product_manufacture_month',\n",
        "    'month_of_year', 'product_expiry_date_year', 'product_manufacture_year',\n",
        "    'transaction_year', 'product_expiry_date_month', 'transaction_month',\n",
        "    'high_value_purchase', 'week_of_year', 'promotion_start_month', 'day_of_week',\n",
        "    'purchase_frequency', 'customer_city', 'gender', 'weekend', 'store_state',\n",
        "    'email_subscriptions', 'store_location', 'high_value_quantity',\n",
        "    # Numeric attributes\n",
        "    'customer_support_calls', 'product_review_count', 'days_since_last_purchase',\n",
        "    'online_purchases', 'distance_to_store', 'product_rating', 'total_transactions',\n",
        "    'product_weight', 'total_items_purchased', 'unit_price', 'total_returned_value',\n",
        "    'membership_years', 'discount_applied', 'avg_discount_used', 'product_shelf_life',\n",
        "    'total_returned_items', 'transaction_hour', 'min_single_purchase_value',\n",
        "    'number_of_children', 'product_stock', 'avg_purchase_value',\n",
        "    'avg_items_per_transaction', 'website_visits', 'age', 'max_single_purchase_value',\n",
        "    'avg_spent_per_category', 'total_discounts_received', 'product_return_rate',\n",
        "    'avg_transaction_value', 'in_store_purchases'\n",
        "]\n",
        "\n",
        "# Filter features that exist in train_data\n",
        "selected_features = [col for col in selected_features if col in train_data.columns]\n",
        "print(\"Initial selected features:\", selected_features)\n",
        "\n",
        "# Combine train_data and test_data for balancing\n",
        "data = pd.concat([train_data, test_data], axis=0)\n",
        "X = data[selected_features]\n",
        "y = data['cluster']\n",
        "\n",
        "# Print class distribution before balancing\n",
        "print(\"Class Distribution Before Balancing:\", Counter(y))\n",
        "\n",
        "# Identify categorical and numeric columns among selected features\n",
        "categorical_cols = [col for col in selected_features if col in ['last_purchase_month', 'promotion_end_month', 'product_manufacture_month', 'month_of_year', 'product_expiry_date_year',\n",
        "                                                                'product_manufacture_year', 'transaction_year', 'product_expiry_date_month', 'transaction_month', 'high_value_purchase',\n",
        "                                                                'week_of_year', 'promotion_start_month', 'day_of_week', 'purchase_frequency', 'customer_city', 'gender', 'weekend', 'store_state',\n",
        "                                                                'email_subscriptions', 'store_location', 'high_value_quantity']]\n",
        "numeric_cols = [col for col in selected_features if col not in categorical_cols]\n",
        "\n",
        "# Preprocessing pipeline for encoding and scaling\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='mean')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ]), numeric_cols),\n",
        "    ('cat', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False))\n",
        "    ]), categorical_cols)\n",
        "])\n",
        "\n",
        "# Preprocess the data before balancing to avoid string-to-float issues\n",
        "X_transformed = preprocessor.fit_transform(X)\n",
        "feature_names = preprocessor.get_feature_names_out()\n",
        "\n",
        "# Define balancing strategy: target 2,000 samples per cluster\n",
        "sampling_strategy_under = {1: 10000}\n",
        "sampling_strategy_over = {0: 10000, 2: 10000}\n",
        "\n",
        "# Create a pipeline for undersampling and oversampling\n",
        "balancing_pipeline = ImbPipeline([\n",
        "    ('undersample', RandomUnderSampler(sampling_strategy=sampling_strategy_under, random_state=42)),\n",
        "    ('oversample', SMOTE(sampling_strategy=sampling_strategy_over, random_state=42))\n",
        "])\n",
        "\n",
        "# Apply balancing\n",
        "X_resampled, y_resampled = balancing_pipeline.fit_resample(X_transformed, y)\n",
        "\n",
        "# Check dataset distribution after balancing\n",
        "print(\"After Balancing:\", Counter(y_resampled))\n",
        "\n",
        "# Split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42)\n",
        "\n",
        "# Convert balanced data to DataFrame for feature selection\n",
        "X_train_df = pd.DataFrame(X_train, columns=feature_names)\n",
        "X_test_df = pd.DataFrame(X_test, columns=feature_names)\n",
        "\n",
        "# Feature selection using Random Forest to get feature importance\n",
        "base_model = RandomForestClassifier(n_estimators=50, random_state=42)\n",
        "base_model.fit(X_train_df, y_train)\n",
        "feature_importances = base_model.feature_importances_\n",
        "top_features_idx = np.argsort(feature_importances)[-10:]  # Get the top 10 features\n",
        "top_10_features = feature_names[top_features_idx]\n",
        "print(f\"Top 10 Features: {list(top_10_features)}\")\n",
        "\n",
        "# Subset data with selected features\n",
        "X_train_selected = X_train_df[top_10_features]\n",
        "X_test_selected = X_test_df[top_10_features]\n",
        "\n",
        "# Create pipeline with KNN (no preprocessing needed since data is pre-transformed)\n",
        "pipeline = SKPipeline([\n",
        "    ('classifier', KNeighborsClassifier())\n",
        "])\n",
        "\n",
        "# Define parameter distribution for RandomizedSearchCV\n",
        "param_dist = {\n",
        "    'classifier__n_neighbors': randint(3, 20),\n",
        "    'classifier__weights': ['uniform', 'distance'],\n",
        "    'classifier__metric': ['euclidean', 'manhattan', 'minkowski']\n",
        "}\n",
        "\n",
        "# Perform RandomizedSearchCV for hyperparameter tuning\n",
        "random_search = RandomizedSearchCV(pipeline, param_distributions=param_dist, n_iter=10, cv=5,\n",
        "                                   scoring='accuracy', n_jobs=-1, random_state=42)\n",
        "random_search.fit(X_train_selected, y_train)\n",
        "\n",
        "# Best hyperparameters\n",
        "best_params = random_search.best_params_\n",
        "print(f\"Best Hyperparameters: {best_params}\")\n",
        "\n",
        "# Train KNN with the best parameters\n",
        "best_model = random_search.best_estimator_\n",
        "best_model.fit(X_train_selected, y_train)\n",
        "\n",
        "# Predictions on test set\n",
        "y_pred = best_model.predict(X_test_selected)\n",
        "\n",
        "# Get unique cluster labels for classification report\n",
        "unique_clusters = sorted(np.unique(y_train))\n",
        "target_names = [f'Cluster {i}' for i in unique_clusters]\n",
        "\n",
        "# Classification Report and Metrics\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=target_names))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# Accuracy, Recall, Precision\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Calculate Specificity for each cluster\n",
        "def calculate_specificity(class_idx):\n",
        "    tn = cm.sum() - cm[:, class_idx].sum() - cm[class_idx, :].sum() + cm[class_idx, class_idx]\n",
        "    fp = cm[:, class_idx].sum() - cm[class_idx, class_idx]\n",
        "    return tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "\n",
        "# Calculate specificity for each cluster\n",
        "specificity_scores = {f'Cluster {i}': calculate_specificity(idx) for idx, i in enumerate(unique_clusters)}\n",
        "\n",
        "# Print metrics\n",
        "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "for cluster, spec in specificity_scores.items():\n",
        "    print(f\"Specificity for {cluster}: {spec:.4f}\")\n",
        "\n",
        "print(\"-----------------------------------------------\")\n",
        "\n",
        "# --- Cross-Validation on Full Dataset ---\n",
        "# Combine train and test data for cross-validation\n",
        "X_full = pd.concat([X_train_selected, X_test_selected], axis=0)\n",
        "y_full = pd.concat([y_train, y_test], axis=0)\n",
        "\n",
        "# Perform 5-fold cross-validation using the best model\n",
        "cv_scores = cross_val_score(best_model, X_full, y_full, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "print(f\"\\nCross-Validation Results (5-fold):\")\n",
        "print(f\"Mean Accuracy: {cv_scores.mean():.4f}\")\n",
        "print(f\"Standard Deviation: {cv_scores.std():.4f}\")\n",
        "print(f\"Individual Fold Scores: {cv_scores}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqwAMfJxngkC",
        "outputId": "eb8efc56-8091-43cd-fdfb-5628a5a3f3d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial selected features: ['last_purchase_month', 'promotion_end_month', 'product_manufacture_month', 'month_of_year', 'product_expiry_date_year', 'product_manufacture_year', 'transaction_year', 'product_expiry_date_month', 'transaction_month', 'high_value_purchase', 'week_of_year', 'promotion_start_month', 'day_of_week', 'purchase_frequency', 'customer_city', 'gender', 'weekend', 'store_state', 'email_subscriptions', 'store_location', 'high_value_quantity', 'customer_support_calls', 'product_review_count', 'days_since_last_purchase', 'online_purchases', 'distance_to_store', 'product_rating', 'total_transactions', 'product_weight', 'total_items_purchased', 'unit_price', 'total_returned_value', 'membership_years', 'discount_applied', 'avg_discount_used', 'product_shelf_life', 'total_returned_items', 'transaction_hour', 'min_single_purchase_value', 'number_of_children', 'product_stock', 'avg_purchase_value', 'avg_items_per_transaction', 'website_visits', 'age', 'max_single_purchase_value', 'avg_spent_per_category', 'total_discounts_received', 'product_return_rate', 'avg_transaction_value', 'in_store_purchases']\n",
            "Class Distribution Before Balancing: Counter({1: 10257, 0: 9932, 2: 9811})\n",
            "After Balancing: Counter({0: 10000, 1: 10000, 2: 10000})\n",
            "Top 10 Features: ['num__product_rating', 'num__product_weight', 'num__total_items_purchased', 'num__total_transactions', 'num__unit_price', 'num__online_purchases', 'num__customer_support_calls', 'num__distance_to_store', 'num__days_since_last_purchase', 'num__product_review_count']\n",
            "Best Hyperparameters: {'classifier__metric': 'minkowski', 'classifier__n_neighbors': 17, 'classifier__weights': 'distance'}\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Cluster 0       0.63      0.63      0.63      3007\n",
            "   Cluster 1       0.61      0.60      0.60      2995\n",
            "   Cluster 2       0.59      0.61      0.60      2998\n",
            "\n",
            "    accuracy                           0.61      9000\n",
            "   macro avg       0.61      0.61      0.61      9000\n",
            "weighted avg       0.61      0.61      0.61      9000\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1890  551  566]\n",
            " [ 531 1795  669]\n",
            " [ 577  607 1814]]\n",
            "\n",
            "Accuracy: 0.6110\n",
            "Recall: 0.6110\n",
            "Precision: 0.6111\n",
            "Specificity for Cluster 0: 0.8151\n",
            "Specificity for Cluster 1: 0.8072\n",
            "Specificity for Cluster 2: 0.7942\n",
            "-----------------------------------------------\n",
            "\n",
            "Cross-Validation Results (5-fold):\n",
            "Mean Accuracy: 0.6019\n",
            "Standard Deviation: 0.0071\n",
            "Individual Fold Scores: [0.60233333 0.5915     0.59766667 0.61283333 0.605     ]\n"
          ]
        }
      ]
    }
  ]
}