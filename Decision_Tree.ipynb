{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from pandas import read_csv\n",
        "import pandas as pd\n",
        "\n",
        "# Specify the encoding as detected ('ascii')\n",
        "train_data = pd.read_csv('/content/srsstat_train_data.csv', encoding='ascii')  # Updated encoding to 'ascii'\n",
        "\n",
        "test_data = pd.read_csv('/content/srsstat_test_data.csv', encoding='ascii')  # Updated encoding to 'ascii'"
      ],
      "metadata": {
        "id": "h0aH8rBMgceH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.value_counts('cluster')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "fIDwARDEpyh0",
        "outputId": "6af45f8b-20cf-45c3-af3a-74cb68f2eba6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "cluster\n",
              "1    7202\n",
              "0    6913\n",
              "2    6885\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cluster</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7202</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6885</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISb8jxewh5hH",
        "outputId": "492c2ea8-0429-44f2-f211-b524c67afb57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 21000 entries, 0 to 20999\n",
            "Data columns (total 87 columns):\n",
            " #   Column                     Non-Null Count  Dtype  \n",
            "---  ------                     --------------  -----  \n",
            " 0   customer_id                21000 non-null  int64  \n",
            " 1   age                        21000 non-null  int64  \n",
            " 2   gender                     21000 non-null  object \n",
            " 3   income_bracket             21000 non-null  object \n",
            " 4   loyalty_program            21000 non-null  object \n",
            " 5   membership_years           21000 non-null  int64  \n",
            " 6   churned                    21000 non-null  object \n",
            " 7   marital_status             21000 non-null  object \n",
            " 8   number_of_children         21000 non-null  int64  \n",
            " 9   education_level            21000 non-null  object \n",
            " 10  occupation                 21000 non-null  object \n",
            " 11  transaction_id             21000 non-null  int64  \n",
            " 12  product_id                 21000 non-null  int64  \n",
            " 13  product_category           21000 non-null  object \n",
            " 14  quantity                   21000 non-null  int64  \n",
            " 15  unit_price                 21000 non-null  float64\n",
            " 16  discount_applied           21000 non-null  float64\n",
            " 17  payment_method             21000 non-null  object \n",
            " 18  store_location             21000 non-null  object \n",
            " 19  transaction_hour           21000 non-null  int64  \n",
            " 20  day_of_week                21000 non-null  object \n",
            " 21  week_of_year               21000 non-null  int64  \n",
            " 22  month_of_year              21000 non-null  int64  \n",
            " 23  avg_purchase_value         21000 non-null  float64\n",
            " 24  purchase_frequency         21000 non-null  object \n",
            " 25  avg_discount_used          21000 non-null  float64\n",
            " 26  preferred_store            21000 non-null  object \n",
            " 27  online_purchases           21000 non-null  int64  \n",
            " 28  in_store_purchases         21000 non-null  int64  \n",
            " 29  avg_items_per_transaction  21000 non-null  float64\n",
            " 30  avg_transaction_value      21000 non-null  float64\n",
            " 31  total_returned_items       21000 non-null  int64  \n",
            " 32  total_returned_value       21000 non-null  float64\n",
            " 33  total_sales_over_lastyear  21000 non-null  float64\n",
            " 34  total_transactions         21000 non-null  int64  \n",
            " 35  total_items_purchased      21000 non-null  int64  \n",
            " 36  total_discounts_received   21000 non-null  float64\n",
            " 37  avg_spent_per_category     21000 non-null  float64\n",
            " 38  max_single_purchase_value  21000 non-null  float64\n",
            " 39  min_single_purchase_value  21000 non-null  float64\n",
            " 40  product_name               21000 non-null  object \n",
            " 41  product_brand              21000 non-null  object \n",
            " 42  product_rating             21000 non-null  float64\n",
            " 43  product_review_count       21000 non-null  int64  \n",
            " 44  product_stock              21000 non-null  int64  \n",
            " 45  product_return_rate        21000 non-null  float64\n",
            " 46  product_size               21000 non-null  object \n",
            " 47  product_weight             21000 non-null  float64\n",
            " 48  product_color              21000 non-null  object \n",
            " 49  product_material           21000 non-null  object \n",
            " 50  product_shelf_life         21000 non-null  int64  \n",
            " 51  promotion_id               21000 non-null  int64  \n",
            " 52  promotion_type             21000 non-null  object \n",
            " 53  promotion_effectiveness    21000 non-null  object \n",
            " 54  promotion_channel          21000 non-null  object \n",
            " 55  promotion_target_audience  21000 non-null  object \n",
            " 56  customer_zip_code          21000 non-null  int64  \n",
            " 57  customer_city              21000 non-null  object \n",
            " 58  customer_state             21000 non-null  object \n",
            " 59  store_zip_code             21000 non-null  int64  \n",
            " 60  store_city                 21000 non-null  object \n",
            " 61  store_state                21000 non-null  object \n",
            " 62  distance_to_store          21000 non-null  float64\n",
            " 63  holiday_season             21000 non-null  object \n",
            " 64  season                     21000 non-null  object \n",
            " 65  weekend                    21000 non-null  object \n",
            " 66  customer_support_calls     21000 non-null  int64  \n",
            " 67  email_subscriptions        21000 non-null  object \n",
            " 68  app_usage                  21000 non-null  object \n",
            " 69  website_visits             21000 non-null  int64  \n",
            " 70  social_media_engagement    21000 non-null  object \n",
            " 71  days_since_last_purchase   21000 non-null  int64  \n",
            " 72  high_value_purchase        21000 non-null  bool   \n",
            " 73  high_value_quantity        21000 non-null  bool   \n",
            " 74  transaction_month          21000 non-null  int64  \n",
            " 75  transaction_year           21000 non-null  int64  \n",
            " 76  product_expiry_date_month  21000 non-null  int64  \n",
            " 77  product_expiry_date_year   21000 non-null  int64  \n",
            " 78  product_manufacture_month  21000 non-null  int64  \n",
            " 79  product_manufacture_year   21000 non-null  int64  \n",
            " 80  promotion_end_month        21000 non-null  int64  \n",
            " 81  promotion_end_year         21000 non-null  int64  \n",
            " 82  last_purchase_month        21000 non-null  int64  \n",
            " 83  last_purchase_year         21000 non-null  int64  \n",
            " 84  promotion_start_month      21000 non-null  int64  \n",
            " 85  promotion_start_year       21000 non-null  int64  \n",
            " 86  cluster                    21000 non-null  int64  \n",
            "dtypes: bool(2), float64(16), int64(37), object(32)\n",
            "memory usage: 13.7+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree - Without Balancing - All Features"
      ],
      "metadata": {
        "id": "x6rnYfmhG6gv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsU9Z1ydGz_7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e8f150a-6ed6-4355-8dd8-a5c866ddd584"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected features for Decision Tree: ['last_purchase_month', 'promotion_end_month', 'product_manufacture_month', 'month_of_year', 'product_expiry_date_year', 'product_manufacture_year', 'transaction_year', 'product_expiry_date_month', 'transaction_month', 'high_value_purchase', 'week_of_year', 'promotion_start_month', 'day_of_week', 'purchase_frequency', 'customer_city', 'gender', 'weekend', 'store_state', 'email_subscriptions', 'store_location', 'high_value_quantity', 'customer_support_calls', 'product_review_count', 'days_since_last_purchase', 'online_purchases', 'distance_to_store', 'product_rating', 'total_transactions', 'product_weight', 'total_items_purchased', 'unit_price', 'total_returned_value', 'membership_years', 'discount_applied', 'avg_discount_used', 'product_shelf_life', 'total_returned_items', 'transaction_hour', 'min_single_purchase_value', 'number_of_children', 'product_stock', 'avg_purchase_value', 'avg_items_per_transaction', 'website_visits', 'age', 'max_single_purchase_value', 'avg_spent_per_category', 'total_discounts_received', 'product_return_rate', 'avg_transaction_value', 'in_store_purchases']\n",
            "Best Hyperparameters: {'classifier__criterion': 'gini', 'classifier__max_depth': 10, 'classifier__min_samples_leaf': 8, 'classifier__min_samples_split': 2}\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Cluster 0       0.53      0.53      0.53      3019\n",
            "   Cluster 1       0.54      0.54      0.54      3055\n",
            "   Cluster 2       0.53      0.52      0.53      2926\n",
            "\n",
            "    accuracy                           0.53      9000\n",
            "   macro avg       0.53      0.53      0.53      9000\n",
            "weighted avg       0.53      0.53      0.53      9000\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1614  732  673]\n",
            " [ 737 1658  660]\n",
            " [ 710  685 1531]]\n",
            "\n",
            "Accuracy: 0.5337\n",
            "Recall: 0.5337\n",
            "Precision: 0.5337\n",
            "Specificity for Cluster 0: 0.7581\n",
            "Specificity for Cluster 1: 0.7616\n",
            "Specificity for Cluster 2: 0.7805\n",
            "-----------------------------------------------\n",
            "\n",
            "Cross-Validation Results (5-fold):\n",
            "Mean Accuracy: 0.5384\n",
            "Standard Deviation: 0.0066\n",
            "Individual Fold Scores: [0.53933333 0.52683333 0.54616667 0.543      0.5365    ]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score  # Added cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, recall_score, precision_score, confusion_matrix\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Verify train_data and test_data exist from previous steps\n",
        "if 'train_data' not in globals() or 'test_data' not in globals():\n",
        "    raise ValueError(\"train_data or test_data not found! Ensure previous steps are executed.\")\n",
        "\n",
        "# Define the attributes\n",
        "selected_features = [\n",
        "    # Categorical attributes\n",
        "    'last_purchase_month', 'promotion_end_month', 'product_manufacture_month',\n",
        "    'month_of_year', 'product_expiry_date_year', 'product_manufacture_year',\n",
        "    'transaction_year', 'product_expiry_date_month', 'transaction_month',\n",
        "    'high_value_purchase', 'week_of_year', 'promotion_start_month', 'day_of_week',\n",
        "    'purchase_frequency', 'customer_city', 'gender', 'weekend', 'store_state',\n",
        "    'email_subscriptions', 'store_location', 'high_value_quantity',\n",
        "    # Numeric attributes\n",
        "    'customer_support_calls', 'product_review_count', 'days_since_last_purchase',\n",
        "    'online_purchases', 'distance_to_store', 'product_rating', 'total_transactions',\n",
        "    'product_weight', 'total_items_purchased', 'unit_price', 'total_returned_value',\n",
        "    'membership_years', 'discount_applied', 'avg_discount_used', 'product_shelf_life',\n",
        "    'total_returned_items', 'transaction_hour', 'min_single_purchase_value',\n",
        "    'number_of_children', 'product_stock', 'avg_purchase_value',\n",
        "    'avg_items_per_transaction', 'website_visits', 'age', 'max_single_purchase_value',\n",
        "    'avg_spent_per_category', 'total_discounts_received', 'product_return_rate',\n",
        "    'avg_transaction_value', 'in_store_purchases'\n",
        "]\n",
        "\n",
        "# Filter features that exist in train_data\n",
        "selected_features = [col for col in selected_features if col in train_data.columns]\n",
        "print(\"Selected features for Decision Tree:\", selected_features)\n",
        "\n",
        "# Define features (X) and target (y) for train and test sets\n",
        "X_train = train_data[selected_features]\n",
        "y_train = train_data['cluster']\n",
        "X_test = test_data[selected_features]\n",
        "y_test = test_data['cluster']\n",
        "\n",
        "# Identify categorical and numeric columns among selected features\n",
        "categorical_cols = [col for col in selected_features if col in ['last_purchase_month', 'promotion_end_month', 'product_manufacture_month', 'month_of_year', 'product_expiry_date_year',\n",
        "                                                                'product_manufacture_year', 'transaction_year', 'product_expiry_date_month', 'transaction_month', 'high_value_purchase',\n",
        "                                                                'week_of_year', 'promotion_start_month', 'day_of_week', 'purchase_frequency', 'customer_city', 'gender', 'weekend', 'store_state',\n",
        "                                                                'email_subscriptions', 'store_location', 'high_value_quantity']]\n",
        "numeric_cols = [col for col in selected_features if col not in categorical_cols]\n",
        "\n",
        "# Preprocessing pipeline for encoding and scaling\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='mean')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ]), numeric_cols),\n",
        "    ('cat', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False))\n",
        "    ]), categorical_cols)\n",
        "])\n",
        "\n",
        "# Create pipeline with preprocessing and Decision Tree\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', DecisionTreeClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "# Hyperparameter tuning using GridSearchCV\n",
        "param_grid = {\n",
        "    'classifier__max_depth': [None, 10, 15, 20, 30],\n",
        "    'classifier__min_samples_split': [2, 5, 10, 15],\n",
        "    'classifier__min_samples_leaf': [1, 2, 4, 8],\n",
        "    'classifier__criterion': ['gini']\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best hyperparameters\n",
        "best_params = grid_search.best_params_\n",
        "print(f\"Best Hyperparameters: {best_params}\")\n",
        "\n",
        "# Train Decision Tree with the best parameters\n",
        "best_model = grid_search.best_estimator_\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions on test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Get unique cluster labels for classification report\n",
        "unique_clusters = sorted(y_train.unique())\n",
        "target_names = [f'Cluster {i}' for i in unique_clusters]\n",
        "\n",
        "# Classification Report and Metrics\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=target_names))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# Accuracy, Recall, Precision\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Calculate Specificity for each cluster\n",
        "def calculate_specificity(class_idx):\n",
        "    # True Negatives for the current class\n",
        "    tn = cm.sum() - cm[:, class_idx].sum() - cm[class_idx, :].sum() + cm[class_idx, class_idx]\n",
        "    # False Positives for the current class\n",
        "    fp = cm[:, class_idx].sum() - cm[class_idx, class_idx]\n",
        "    # Specificity = TN / (TN + FP)\n",
        "    return tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "\n",
        "# Calculate specificity for each cluster\n",
        "specificity_scores = {f'Cluster {i}': calculate_specificity(idx) for idx, i in enumerate(unique_clusters)}\n",
        "\n",
        "# Print metrics\n",
        "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "for cluster, spec in specificity_scores.items():\n",
        "    print(f\"Specificity for {cluster}: {spec:.4f}\")\n",
        "\n",
        "print(\"-----------------------------------------------\")\n",
        "\n",
        "# --- Cross-Validation on Full Dataset ---\n",
        "# Combine train and test data for cross-validation\n",
        "X_full = pd.concat([X_train, X_test], axis=0)\n",
        "y_full = pd.concat([y_train, y_test], axis=0)\n",
        "\n",
        "# Perform 5-fold cross-validation using the best model\n",
        "cv_scores = cross_val_score(best_model, X_full, y_full, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "print(f\"\\nCross-Validation Results (5-fold):\")\n",
        "print(f\"Mean Accuracy: {cv_scores.mean():.4f}\")\n",
        "print(f\"Standard Deviation: {cv_scores.std():.4f}\")\n",
        "print(f\"Individual Fold Scores: {cv_scores}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree - Without Balancing - Selected Features"
      ],
      "metadata": {
        "id": "jEdZh9TNHG9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.metrics import classification_report, accuracy_score, recall_score, precision_score, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Verify train_data and test_data exist from previous steps\n",
        "if 'train_data' not in globals() or 'test_data' not in globals():\n",
        "    raise ValueError(\"train_data or test_data not found! Ensure previous steps are executed.\")\n",
        "\n",
        "# Define the attributes as specified\n",
        "selected_features = [\n",
        "    # Categorical attributes\n",
        "    'last_purchase_month', 'promotion_end_month', 'product_manufacture_month',\n",
        "    'month_of_year', 'product_expiry_date_year', 'product_manufacture_year',\n",
        "    'transaction_year', 'product_expiry_date_month', 'transaction_month',\n",
        "    'high_value_purchase', 'week_of_year', 'promotion_start_month', 'day_of_week',\n",
        "    'purchase_frequency', 'customer_city', 'gender', 'weekend', 'store_state',\n",
        "    'email_subscriptions', 'store_location', 'high_value_quantity',\n",
        "    # Numeric attributes\n",
        "    'customer_support_calls', 'product_review_count', 'days_since_last_purchase',\n",
        "    'online_purchases', 'distance_to_store', 'product_rating', 'total_transactions',\n",
        "    'product_weight', 'total_items_purchased', 'unit_price', 'total_returned_value',\n",
        "    'membership_years', 'discount_applied', 'avg_discount_used', 'product_shelf_life',\n",
        "    'total_returned_items', 'transaction_hour', 'min_single_purchase_value',\n",
        "    'number_of_children', 'product_stock', 'avg_purchase_value',\n",
        "    'avg_items_per_transaction', 'website_visits', 'age', 'max_single_purchase_value',\n",
        "    'avg_spent_per_category', 'total_discounts_received', 'product_return_rate',\n",
        "    'avg_transaction_value', 'in_store_purchases'\n",
        "]\n",
        "\n",
        "# Filter features that exist in train_data\n",
        "selected_features = [col for col in selected_features if col in train_data.columns]\n",
        "print(\"Initial selected features:\", selected_features)\n",
        "\n",
        "# Define features (X) and target (y) for train and test sets\n",
        "X_train = train_data[selected_features]\n",
        "y_train = train_data['cluster']\n",
        "X_test = test_data[selected_features]\n",
        "y_test = test_data['cluster']\n",
        "\n",
        "# Identify categorical and numeric columns among selected features\n",
        "categorical_cols = [col for col in selected_features if col in ['last_purchase_month', 'promotion_end_month', 'product_manufacture_month', 'month_of_year', 'product_expiry_date_year',\n",
        "                                                                'product_manufacture_year', 'transaction_year', 'product_expiry_date_month', 'transaction_month', 'high_value_purchase',\n",
        "                                                                'week_of_year', 'promotion_start_month', 'day_of_week', 'purchase_frequency', 'customer_city', 'gender', 'weekend', 'store_state',\n",
        "                                                                'email_subscriptions', 'store_location', 'high_value_quantity']]\n",
        "numeric_cols = [col for col in selected_features if col not in categorical_cols]\n",
        "\n",
        "\n",
        "# Preprocessing pipeline for encoding and scaling\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='mean')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ]), numeric_cols),\n",
        "    ('cat', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False))\n",
        "    ]), categorical_cols)\n",
        "])\n",
        "\n",
        "# Preprocess the data before RFECV to avoid string-to-float issues\n",
        "X_train_transformed = preprocessor.fit_transform(X_train)\n",
        "X_test_transformed = preprocessor.transform(X_test)\n",
        "\n",
        "# Get transformed feature names\n",
        "feature_names = preprocessor.get_feature_names_out()\n",
        "\n",
        "# Initialize RFECV with a simple Decision Tree (no preprocessing needed)\n",
        "rfecv = RFECV(estimator=DecisionTreeClassifier(random_state=42), step=1, cv=3, scoring='accuracy', n_jobs=-1)\n",
        "rfecv.fit(X_train_transformed, y_train)\n",
        "\n",
        "# Get selected features from transformed space\n",
        "selected_mask = rfecv.support_\n",
        "selected_transformed_features = feature_names[selected_mask].tolist()\n",
        "print(f\"Number of Features Selected by RFECV: {len(selected_transformed_features)}\")\n",
        "print(f\"Selected Transformed Features: {selected_transformed_features}\")\n",
        "\n",
        "# Map transformed features back to original features\n",
        "# Since one-hot encoding creates multiple columns per categorical feature, we extract unique original feature names\n",
        "original_selected_features = []\n",
        "for transformed_feature in selected_transformed_features:\n",
        "    # Remove prefixes like 'num__' or 'cat__'\n",
        "    if transformed_feature.startswith('num__'):\n",
        "        original_feature = transformed_feature.replace('num__', '')\n",
        "    elif transformed_feature.startswith('cat__'):\n",
        "        # Extract the original feature name before the category (e.g., 'promotion_effectiveness_Low' -> 'promotion_effectiveness')\n",
        "        original_feature = transformed_feature.replace('cat__', '').split('_')[0]\n",
        "    else:\n",
        "        original_feature = transformed_feature\n",
        "    if original_feature in selected_features and original_feature not in original_selected_features:\n",
        "        original_selected_features.append(original_feature)\n",
        "\n",
        "print(f\"Selected Original Features: {original_selected_features}\")\n",
        "\n",
        "# Use selected original features for training and testing\n",
        "X_train_rfe = X_train[original_selected_features]\n",
        "X_test_rfe = X_test[original_selected_features]\n",
        "\n",
        "# Update categorical and numeric columns for the selected features\n",
        "categorical_cols_rfe = [col for col in original_selected_features if col in categorical_cols]\n",
        "numeric_cols_rfe = [col for col in original_selected_features if col not in categorical_cols]\n",
        "\n",
        "# Update preprocessing pipeline for selected features\n",
        "preprocessor_rfe = ColumnTransformer(transformers=[\n",
        "    ('num', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='mean')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ]), numeric_cols_rfe),\n",
        "    ('cat', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False))\n",
        "    ]), categorical_cols_rfe)\n",
        "])\n",
        "\n",
        "# Create pipeline with preprocessing and Decision Tree\n",
        "pipeline_rfe = Pipeline([\n",
        "    ('preprocessor', preprocessor_rfe),\n",
        "    ('classifier', DecisionTreeClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "# Hyperparameter tuning using GridSearchCV\n",
        "param_grid = {\n",
        "    'classifier__max_depth': [None, 10, 15, 20, 30],\n",
        "    'classifier__min_samples_split': [2, 5, 10, 15],\n",
        "    'classifier__min_samples_leaf': [1, 2, 4, 8],\n",
        "    'classifier__criterion': ['gini']\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(pipeline_rfe, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(X_train_rfe, y_train)\n",
        "\n",
        "# Best hyperparameters\n",
        "best_params = grid_search.best_params_\n",
        "print(f\"Best Hyperparameters: {best_params}\")\n",
        "\n",
        "# Train Decision Tree with the best parameters\n",
        "best_model = grid_search.best_estimator_\n",
        "best_model.fit(X_train_rfe, y_train)\n",
        "\n",
        "# Predictions on test set\n",
        "y_pred = best_model.predict(X_test_rfe)\n",
        "\n",
        "# Get unique cluster labels for classification report\n",
        "unique_clusters = sorted(y_train.unique())\n",
        "target_names = [f'Cluster {i}' for i in unique_clusters]\n",
        "\n",
        "# Classification Report and Metrics\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=target_names))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# Accuracy, Recall, Precision\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Calculate Specificity for each cluster\n",
        "def calculate_specificity(class_idx):\n",
        "    # True Negatives for the current class\n",
        "    tn = cm.sum() - cm[:, class_idx].sum() - cm[class_idx, :].sum() + cm[class_idx, class_idx]\n",
        "    # False Positives for the current class\n",
        "    fp = cm[:, class_idx].sum() - cm[class_idx, class_idx]\n",
        "    # Specificity = TN / (TN + FP)\n",
        "    return tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "\n",
        "# Calculate specificity for each cluster\n",
        "specificity_scores = {f'Cluster {i}': calculate_specificity(idx) for idx, i in enumerate(unique_clusters)}\n",
        "\n",
        "# Print metrics\n",
        "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "for cluster, spec in specificity_scores.items():\n",
        "    print(f\"Specificity for {cluster}: {spec:.4f}\")\n",
        "\n",
        "print(\"-----------------------------------------------\")\n",
        "\n",
        "# --- Cross-Validation on Full Dataset ---\n",
        "# Combine train and test data for cross-validation\n",
        "X_full = pd.concat([X_train_rfe, X_test_rfe], axis=0)\n",
        "y_full = pd.concat([y_train, y_test], axis=0)\n",
        "\n",
        "# Perform 5-fold cross-validation using the best model\n",
        "cv_scores = cross_val_score(best_model, X_full, y_full, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "print(f\"\\nCross-Validation Results (5-fold):\")\n",
        "print(f\"Mean Accuracy: {cv_scores.mean():.4f}\")\n",
        "print(f\"Standard Deviation: {cv_scores.std():.4f}\")\n",
        "print(f\"Individual Fold Scores: {cv_scores}\")"
      ],
      "metadata": {
        "id": "L8mlRyiqHGvV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27155472-90b3-4c92-89bc-2516118ba3d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial selected features: ['last_purchase_month', 'promotion_end_month', 'product_manufacture_month', 'month_of_year', 'product_expiry_date_year', 'product_manufacture_year', 'transaction_year', 'product_expiry_date_month', 'transaction_month', 'high_value_purchase', 'week_of_year', 'promotion_start_month', 'day_of_week', 'purchase_frequency', 'customer_city', 'gender', 'weekend', 'store_state', 'email_subscriptions', 'store_location', 'high_value_quantity', 'customer_support_calls', 'product_review_count', 'days_since_last_purchase', 'online_purchases', 'distance_to_store', 'product_rating', 'total_transactions', 'product_weight', 'total_items_purchased', 'unit_price', 'total_returned_value', 'membership_years', 'discount_applied', 'avg_discount_used', 'product_shelf_life', 'total_returned_items', 'transaction_hour', 'min_single_purchase_value', 'number_of_children', 'product_stock', 'avg_purchase_value', 'avg_items_per_transaction', 'website_visits', 'age', 'max_single_purchase_value', 'avg_spent_per_category', 'total_discounts_received', 'product_return_rate', 'avg_transaction_value', 'in_store_purchases']\n",
            "Number of Features Selected by RFECV: 134\n",
            "Selected Transformed Features: ['num__customer_support_calls', 'num__product_review_count', 'num__days_since_last_purchase', 'num__online_purchases', 'num__distance_to_store', 'num__product_rating', 'num__total_transactions', 'num__product_weight', 'num__total_items_purchased', 'num__unit_price', 'num__total_returned_value', 'num__membership_years', 'num__discount_applied', 'num__avg_discount_used', 'num__product_shelf_life', 'num__total_returned_items', 'num__transaction_hour', 'num__min_single_purchase_value', 'num__number_of_children', 'num__product_stock', 'num__avg_purchase_value', 'num__avg_items_per_transaction', 'num__website_visits', 'num__age', 'num__max_single_purchase_value', 'num__avg_spent_per_category', 'num__total_discounts_received', 'num__product_return_rate', 'num__avg_transaction_value', 'num__in_store_purchases', 'cat__last_purchase_month_2', 'cat__last_purchase_month_4', 'cat__last_purchase_month_5', 'cat__last_purchase_month_7', 'cat__last_purchase_month_8', 'cat__last_purchase_month_9', 'cat__last_purchase_month_10', 'cat__last_purchase_month_11', 'cat__last_purchase_month_12', 'cat__promotion_end_month_2', 'cat__promotion_end_month_3', 'cat__promotion_end_month_4', 'cat__promotion_end_month_5', 'cat__promotion_end_month_7', 'cat__promotion_end_month_9', 'cat__promotion_end_month_10', 'cat__promotion_end_month_11', 'cat__promotion_end_month_12', 'cat__product_manufacture_month_2', 'cat__product_manufacture_month_3', 'cat__product_manufacture_month_4', 'cat__product_manufacture_month_5', 'cat__product_manufacture_month_6', 'cat__product_manufacture_month_7', 'cat__product_manufacture_month_8', 'cat__product_manufacture_month_9', 'cat__product_manufacture_month_10', 'cat__product_manufacture_month_11', 'cat__product_manufacture_month_12', 'cat__month_of_year_3', 'cat__month_of_year_4', 'cat__month_of_year_5', 'cat__month_of_year_6', 'cat__month_of_year_7', 'cat__month_of_year_8', 'cat__month_of_year_9', 'cat__month_of_year_10', 'cat__month_of_year_11', 'cat__month_of_year_12', 'cat__product_expiry_date_year_2023', 'cat__product_manufacture_year_2019', 'cat__transaction_year_2021', 'cat__product_expiry_date_month_2', 'cat__product_expiry_date_month_3', 'cat__product_expiry_date_month_4', 'cat__product_expiry_date_month_5', 'cat__product_expiry_date_month_6', 'cat__product_expiry_date_month_7', 'cat__product_expiry_date_month_8', 'cat__product_expiry_date_month_9', 'cat__product_expiry_date_month_10', 'cat__product_expiry_date_month_12', 'cat__transaction_month_2', 'cat__transaction_month_3', 'cat__transaction_month_5', 'cat__transaction_month_6', 'cat__transaction_month_7', 'cat__transaction_month_8', 'cat__transaction_month_11', 'cat__transaction_month_12', 'cat__high_value_purchase_True', 'cat__week_of_year_4', 'cat__week_of_year_5', 'cat__week_of_year_6', 'cat__week_of_year_8', 'cat__week_of_year_9', 'cat__week_of_year_11', 'cat__week_of_year_12', 'cat__week_of_year_16', 'cat__week_of_year_22', 'cat__week_of_year_40', 'cat__week_of_year_42', 'cat__week_of_year_48', 'cat__week_of_year_49', 'cat__week_of_year_50', 'cat__week_of_year_51', 'cat__promotion_start_month_3', 'cat__promotion_start_month_5', 'cat__promotion_start_month_6', 'cat__promotion_start_month_7', 'cat__promotion_start_month_8', 'cat__promotion_start_month_9', 'cat__promotion_start_month_10', 'cat__promotion_start_month_11', 'cat__promotion_start_month_12', 'cat__day_of_week_Saturday', 'cat__day_of_week_Thursday', 'cat__day_of_week_Wednesday', 'cat__purchase_frequency_Monthly', 'cat__purchase_frequency_Weekly', 'cat__purchase_frequency_Yearly', 'cat__customer_city_City B', 'cat__customer_city_City C', 'cat__customer_city_City D', 'cat__gender_Male', 'cat__gender_Other', 'cat__weekend_Yes', 'cat__store_state_State Y', 'cat__store_state_State Z', 'cat__email_subscriptions_Yes', 'cat__store_location_Location B', 'cat__store_location_Location C', 'cat__store_location_Location D', 'cat__high_value_quantity_True']\n",
            "Selected Original Features: ['customer_support_calls', 'product_review_count', 'days_since_last_purchase', 'online_purchases', 'distance_to_store', 'product_rating', 'total_transactions', 'product_weight', 'total_items_purchased', 'unit_price', 'total_returned_value', 'membership_years', 'discount_applied', 'avg_discount_used', 'product_shelf_life', 'total_returned_items', 'transaction_hour', 'min_single_purchase_value', 'number_of_children', 'product_stock', 'avg_purchase_value', 'avg_items_per_transaction', 'website_visits', 'age', 'max_single_purchase_value', 'avg_spent_per_category', 'total_discounts_received', 'product_return_rate', 'avg_transaction_value', 'in_store_purchases', 'gender', 'weekend']\n",
            "Best Hyperparameters: {'classifier__criterion': 'gini', 'classifier__max_depth': 10, 'classifier__min_samples_leaf': 4, 'classifier__min_samples_split': 15}\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Cluster 0       0.52      0.53      0.53      3019\n",
            "   Cluster 1       0.53      0.53      0.53      3055\n",
            "   Cluster 2       0.53      0.53      0.53      2926\n",
            "\n",
            "    accuracy                           0.53      9000\n",
            "   macro avg       0.53      0.53      0.53      9000\n",
            "weighted avg       0.53      0.53      0.53      9000\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1611  741  667]\n",
            " [ 760 1624  671]\n",
            " [ 701  687 1538]]\n",
            "\n",
            "Accuracy: 0.5303\n",
            "Recall: 0.5303\n",
            "Precision: 0.5304\n",
            "Specificity for Cluster 0: 0.7557\n",
            "Specificity for Cluster 1: 0.7598\n",
            "Specificity for Cluster 2: 0.7797\n",
            "-----------------------------------------------\n",
            "\n",
            "Cross-Validation Results (5-fold):\n",
            "Mean Accuracy: 0.5377\n",
            "Standard Deviation: 0.0026\n",
            "Individual Fold Scores: [0.54033333 0.53316667 0.53983333 0.537      0.53816667]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree - With Balancing - All Features"
      ],
      "metadata": {
        "id": "qc0XIhuzHKGh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, recall_score, precision_score, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline as SKPipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Define the attributes as specified\n",
        "selected_features = [\n",
        "    # Categorical attributes\n",
        "    'last_purchase_month', 'promotion_end_month', 'product_manufacture_month',\n",
        "    'month_of_year', 'product_expiry_date_year', 'product_manufacture_year',\n",
        "    'transaction_year', 'product_expiry_date_month', 'transaction_month',\n",
        "    'high_value_purchase', 'week_of_year', 'promotion_start_month', 'day_of_week',\n",
        "    'purchase_frequency', 'customer_city', 'gender', 'weekend', 'store_state',\n",
        "    'email_subscriptions', 'store_location', 'high_value_quantity',\n",
        "    # Numeric attributes\n",
        "    'customer_support_calls', 'product_review_count', 'days_since_last_purchase',\n",
        "    'online_purchases', 'distance_to_store', 'product_rating', 'total_transactions',\n",
        "    'product_weight', 'total_items_purchased', 'unit_price', 'total_returned_value',\n",
        "    'membership_years', 'discount_applied', 'avg_discount_used', 'product_shelf_life',\n",
        "    'total_returned_items', 'transaction_hour', 'min_single_purchase_value',\n",
        "    'number_of_children', 'product_stock', 'avg_purchase_value',\n",
        "    'avg_items_per_transaction', 'website_visits', 'age', 'max_single_purchase_value',\n",
        "    'avg_spent_per_category', 'total_discounts_received', 'product_return_rate',\n",
        "    'avg_transaction_value', 'in_store_purchases'\n",
        "]\n",
        "\n",
        "# Filter features that exist in train_data\n",
        "selected_features = [col for col in selected_features if col in train_data.columns]\n",
        "print(\"Initial selected features:\", selected_features)\n",
        "\n",
        "# Combine train_data and test_data for balancing\n",
        "data = pd.concat([train_data, test_data], axis=0)\n",
        "X = data[selected_features]\n",
        "y = data['cluster']\n",
        "\n",
        "# Identify categorical and numeric columns among selected features\n",
        "categorical_cols = [col for col in selected_features if col in ['last_purchase_month', 'promotion_end_month', 'product_manufacture_month', 'month_of_year', 'product_expiry_date_year',\n",
        "                                                                'product_manufacture_year', 'transaction_year', 'product_expiry_date_month', 'transaction_month', 'high_value_purchase',\n",
        "                                                                'week_of_year', 'promotion_start_month', 'day_of_week', 'purchase_frequency', 'customer_city', 'gender', 'weekend', 'store_state',\n",
        "                                                                'email_subscriptions', 'store_location', 'high_value_quantity']]\n",
        "numeric_cols = [col for col in selected_features if col not in categorical_cols]\n",
        "\n",
        "\n",
        "# Preprocessing pipeline for encoding and scaling\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='mean')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ]), numeric_cols),\n",
        "    ('cat', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False))\n",
        "    ]), categorical_cols)\n",
        "])\n",
        "\n",
        "# Preprocess the data before balancing to avoid string-to-float issues\n",
        "X_transformed = preprocessor.fit_transform(X)\n",
        "feature_names = preprocessor.get_feature_names_out()\n",
        "\n",
        "# Define balancing strategy: target 2,000 samples per cluster\n",
        "sampling_strategy_under = {1: 10000}\n",
        "sampling_strategy_over = {0: 10000, 2: 10000}\n",
        "\n",
        "# Create a pipeline for undersampling and oversampling\n",
        "balancing_pipeline = ImbPipeline([\n",
        "    ('undersample', RandomUnderSampler(sampling_strategy=sampling_strategy_under, random_state=42)),\n",
        "    ('oversample', SMOTE(sampling_strategy=sampling_strategy_over, random_state=42))\n",
        "])\n",
        "\n",
        "# Apply balancing\n",
        "X_resampled, y_resampled = balancing_pipeline.fit_resample(X_transformed, y)\n",
        "\n",
        "# Check dataset distribution after balancing\n",
        "print(\"After Balancing:\", Counter(y_resampled))\n",
        "\n",
        "# Split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create pipeline with Decision Tree (no preprocessing needed since data is pre-transformed)\n",
        "pipeline = SKPipeline([\n",
        "    ('classifier', DecisionTreeClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "# Hyperparameter tuning using GridSearchCV\n",
        "param_grid = {\n",
        "    'classifier__max_depth': [None, 10, 15, 20, 30],\n",
        "    'classifier__min_samples_split': [2, 5, 10, 15],\n",
        "    'classifier__min_samples_leaf': [1, 2, 4, 8],\n",
        "    'classifier__criterion': ['gini']\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best hyperparameters\n",
        "best_params = grid_search.best_params_\n",
        "print(f\"Best Hyperparameters: {best_params}\")\n",
        "\n",
        "# Train Decision Tree with the best parameters\n",
        "best_model = grid_search.best_estimator_\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions on test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Get unique cluster labels for classification report\n",
        "unique_clusters = sorted(np.unique(y_train))\n",
        "target_names = [f'Cluster {i}' for i in unique_clusters]\n",
        "\n",
        "# Classification Report and Metrics\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=target_names))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# Accuracy, Recall, Precision\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Calculate Specificity for each cluster\n",
        "def calculate_specificity(class_idx):\n",
        "    tn = cm.sum() - cm[:, class_idx].sum() - cm[class_idx, :].sum() + cm[class_idx, class_idx]\n",
        "    fp = cm[:, class_idx].sum() - cm[class_idx, class_idx]\n",
        "    return tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "\n",
        "# Calculate specificity for each cluster\n",
        "specificity_scores = {f'Cluster {i}': calculate_specificity(idx) for idx, i in enumerate(unique_clusters)}\n",
        "\n",
        "# Print metrics\n",
        "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "for cluster, spec in specificity_scores.items():\n",
        "    print(f\"Specificity for {cluster}: {spec:.4f}\")\n",
        "\n",
        "print(\"-----------------------------------------------\")\n",
        "\n",
        "# --- Cross-Validation on Full Dataset ---\n",
        "# Combine train and test data for cross-validation\n",
        "X_full = np.vstack([X_train, X_test])\n",
        "y_full = np.concatenate([y_train, y_test])\n",
        "\n",
        "# Perform 5-fold cross-validation using the best model\n",
        "cv_scores = cross_val_score(best_model, X_full, y_full, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "print(f\"\\nCross-Validation Results (5-fold):\")\n",
        "print(f\"Mean Accuracy: {cv_scores.mean():.4f}\")\n",
        "print(f\"Standard Deviation: {cv_scores.std():.4f}\")\n",
        "print(f\"Individual Fold Scores: {cv_scores}\")"
      ],
      "metadata": {
        "id": "83Hnd0LxG-zK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "868f6b82-4dd2-4fa5-eedc-f7b722c9c91c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial selected features: ['last_purchase_month', 'promotion_end_month', 'product_manufacture_month', 'month_of_year', 'product_expiry_date_year', 'product_manufacture_year', 'transaction_year', 'product_expiry_date_month', 'transaction_month', 'high_value_purchase', 'week_of_year', 'promotion_start_month', 'day_of_week', 'purchase_frequency', 'customer_city', 'gender', 'weekend', 'store_state', 'email_subscriptions', 'store_location', 'high_value_quantity', 'customer_support_calls', 'product_review_count', 'days_since_last_purchase', 'online_purchases', 'distance_to_store', 'product_rating', 'total_transactions', 'product_weight', 'total_items_purchased', 'unit_price', 'total_returned_value', 'membership_years', 'discount_applied', 'avg_discount_used', 'product_shelf_life', 'total_returned_items', 'transaction_hour', 'min_single_purchase_value', 'number_of_children', 'product_stock', 'avg_purchase_value', 'avg_items_per_transaction', 'website_visits', 'age', 'max_single_purchase_value', 'avg_spent_per_category', 'total_discounts_received', 'product_return_rate', 'avg_transaction_value', 'in_store_purchases']\n",
            "After Balancing: Counter({0: 10000, 1: 10000, 2: 10000})\n",
            "Best Hyperparameters: {'classifier__criterion': 'gini', 'classifier__max_depth': 10, 'classifier__min_samples_leaf': 8, 'classifier__min_samples_split': 2}\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Cluster 0       0.52      0.55      0.54      3007\n",
            "   Cluster 1       0.54      0.53      0.54      2995\n",
            "   Cluster 2       0.55      0.54      0.54      2998\n",
            "\n",
            "    accuracy                           0.54      9000\n",
            "   macro avg       0.54      0.54      0.54      9000\n",
            "weighted avg       0.54      0.54      0.54      9000\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1644  717  646]\n",
            " [ 740 1598  657]\n",
            " [ 751  640 1607]]\n",
            "\n",
            "Accuracy: 0.5388\n",
            "Recall: 0.5388\n",
            "Precision: 0.5391\n",
            "Specificity for Cluster 0: 0.7512\n",
            "Specificity for Cluster 1: 0.7740\n",
            "Specificity for Cluster 2: 0.7829\n",
            "-----------------------------------------------\n",
            "\n",
            "Cross-Validation Results (5-fold):\n",
            "Mean Accuracy: 0.5422\n",
            "Standard Deviation: 0.0030\n",
            "Individual Fold Scores: [0.54383333 0.54466667 0.5445     0.53666667 0.54133333]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree - With Balancing - Selected Features"
      ],
      "metadata": {
        "id": "cUho4fktHOBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.metrics import classification_report, accuracy_score, recall_score, precision_score, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline as SKPipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Verify train_data and test_data exist from previous steps\n",
        "if 'train_data' not in globals() or 'test_data' not in globals():\n",
        "    raise ValueError(\"train_data or test_data not found! Ensure previous steps are executed.\")\n",
        "\n",
        "# Define the attributes as specified\n",
        "selected_features = [\n",
        "    # Categorical attributes\n",
        "    'last_purchase_month', 'promotion_end_month', 'product_manufacture_month',\n",
        "    'month_of_year', 'product_expiry_date_year', 'product_manufacture_year',\n",
        "    'transaction_year', 'product_expiry_date_month', 'transaction_month',\n",
        "    'high_value_purchase', 'week_of_year', 'promotion_start_month', 'day_of_week',\n",
        "    'purchase_frequency', 'customer_city', 'gender', 'weekend', 'store_state',\n",
        "    'email_subscriptions', 'store_location', 'high_value_quantity',\n",
        "    # Numeric attributes\n",
        "    'customer_support_calls', 'product_review_count', 'days_since_last_purchase',\n",
        "    'online_purchases', 'distance_to_store', 'product_rating', 'total_transactions',\n",
        "    'product_weight', 'total_items_purchased', 'unit_price', 'total_returned_value',\n",
        "    'membership_years', 'discount_applied', 'avg_discount_used', 'product_shelf_life',\n",
        "    'total_returned_items', 'transaction_hour', 'min_single_purchase_value',\n",
        "    'number_of_children', 'product_stock', 'avg_purchase_value',\n",
        "    'avg_items_per_transaction', 'website_visits', 'age', 'max_single_purchase_value',\n",
        "    'avg_spent_per_category', 'total_discounts_received', 'product_return_rate',\n",
        "    'avg_transaction_value', 'in_store_purchases'\n",
        "]\n",
        "\n",
        "# Filter features that exist in train_data\n",
        "selected_features = [col for col in selected_features if col in train_data.columns]\n",
        "print(\"Initial selected features:\", selected_features)\n",
        "\n",
        "# Combine train_data and test_data for balancing\n",
        "data = pd.concat([train_data, test_data], axis=0)\n",
        "X = data[selected_features]\n",
        "y = data['cluster']\n",
        "\n",
        "# Identify categorical and numeric columns among selected features\n",
        "categorical_cols = [col for col in selected_features if col in ['last_purchase_month', 'promotion_end_month', 'product_manufacture_month', 'month_of_year', 'product_expiry_date_year',\n",
        "                                                                'product_manufacture_year', 'transaction_year', 'product_expiry_date_month', 'transaction_month', 'high_value_purchase',\n",
        "                                                                'week_of_year', 'promotion_start_month', 'day_of_week', 'purchase_frequency', 'customer_city', 'gender', 'weekend', 'store_state',\n",
        "                                                                'email_subscriptions', 'store_location', 'high_value_quantity']]\n",
        "numeric_cols = [col for col in selected_features if col not in categorical_cols]\n",
        "\n",
        "\n",
        "# Preprocessing pipeline for encoding and scaling\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='mean')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ]), numeric_cols),\n",
        "    ('cat', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False))\n",
        "    ]), categorical_cols)\n",
        "])\n",
        "\n",
        "# Preprocess the data before balancing to avoid string-to-float issues\n",
        "X_transformed = preprocessor.fit_transform(X)\n",
        "feature_names = preprocessor.get_feature_names_out()\n",
        "\n",
        "# Define balancing strategy: target 2,000 samples per cluster\n",
        "sampling_strategy_under = {1: 10000}\n",
        "sampling_strategy_over = {0: 10000, 2: 10000}\n",
        "\n",
        "# Create a pipeline for undersampling and oversampling\n",
        "balancing_pipeline = ImbPipeline([\n",
        "    ('undersample', RandomUnderSampler(sampling_strategy=sampling_strategy_under, random_state=42)),\n",
        "    ('oversample', SMOTE(sampling_strategy=sampling_strategy_over, random_state=42))\n",
        "])\n",
        "\n",
        "# Apply balancing\n",
        "X_resampled, y_resampled = balancing_pipeline.fit_resample(X_transformed, y)\n",
        "\n",
        "# Check dataset distribution after balancing\n",
        "print(\"After Balancing:\", Counter(y_resampled))\n",
        "\n",
        "# Split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize RFECV with a simple Decision Tree (no preprocessing needed)\n",
        "rfecv = RFECV(estimator=DecisionTreeClassifier(random_state=42), step=1, cv=3, scoring='accuracy', n_jobs=-1)\n",
        "rfecv.fit(X_train, y_train)\n",
        "\n",
        "# Get selected features from transformed space\n",
        "selected_mask = rfecv.support_\n",
        "selected_transformed_features = feature_names[selected_mask].tolist()\n",
        "print(f\"Number of Features Selected by RFECV: {len(selected_transformed_features)}\")\n",
        "print(f\"Selected Transformed Features: {selected_transformed_features}\")\n",
        "\n",
        "# Map transformed features back to original features\n",
        "original_selected_features = []\n",
        "for transformed_feature in selected_transformed_features:\n",
        "    if transformed_feature.startswith('num__'):\n",
        "        original_feature = transformed_feature.replace('num__', '')\n",
        "    elif transformed_feature.startswith('cat__'):\n",
        "        original_feature = transformed_feature.replace('cat__', '').split('_')[0]\n",
        "    else:\n",
        "        original_feature = transformed_feature\n",
        "    if original_feature in selected_features and original_feature not in original_selected_features:\n",
        "        original_selected_features.append(original_feature)\n",
        "\n",
        "print(f\"Selected Original Features: {original_selected_features}\")\n",
        "\n",
        "# Since X_train and X_test are already transformed, we need to select the corresponding transformed feature indices\n",
        "selected_indices = [i for i, name in enumerate(feature_names) if name in selected_transformed_features]\n",
        "X_train_rfe = X_train[:, selected_indices]\n",
        "X_test_rfe = X_test[:, selected_indices]\n",
        "\n",
        "# Create pipeline with Decision Tree (no preprocessing needed since data is pre-transformed)\n",
        "pipeline_rfe = Pipeline([\n",
        "    ('classifier', DecisionTreeClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "# Hyperparameter tuning using GridSearchCV\n",
        "param_grid = {\n",
        "    'classifier__max_depth': [None, 10, 15, 20, 30],\n",
        "    'classifier__min_samples_split': [2, 5, 10, 15],\n",
        "    'classifier__min_samples_leaf': [1, 2, 4, 8],\n",
        "    'classifier__criterion': ['gini']\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(pipeline_rfe, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(X_train_rfe, y_train)\n",
        "\n",
        "# Best hyperparameters\n",
        "best_params = grid_search.best_params_\n",
        "print(f\"Best Hyperparameters: {best_params}\")\n",
        "\n",
        "# Train Decision Tree with the best parameters\n",
        "best_model = grid_search.best_estimator_\n",
        "best_model.fit(X_train_rfe, y_train)\n",
        "\n",
        "# Predictions on test set\n",
        "y_pred = best_model.predict(X_test_rfe)\n",
        "\n",
        "# Get unique cluster labels for classification report\n",
        "unique_clusters = sorted(np.unique(y_train))\n",
        "target_names = [f'Cluster {i}' for i in unique_clusters]\n",
        "\n",
        "# Classification Report and Metrics\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=target_names))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# Accuracy, Recall, Precision\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Calculate Specificity for each cluster\n",
        "def calculate_specificity(class_idx):\n",
        "    tn = cm.sum() - cm[:, class_idx].sum() - cm[class_idx, :].sum() + cm[class_idx, class_idx]\n",
        "    fp = cm[:, class_idx].sum() - cm[class_idx, class_idx]\n",
        "    return tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "\n",
        "# Calculate specificity for each cluster\n",
        "specificity_scores = {f'Cluster {i}': calculate_specificity(idx) for idx, i in enumerate(unique_clusters)}\n",
        "\n",
        "# Print metrics\n",
        "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "for cluster, spec in specificity_scores.items():\n",
        "    print(f\"Specificity for {cluster}: {spec:.4f}\")\n",
        "\n",
        "print(\"-----------------------------------------------\")\n",
        "\n",
        "# --- Cross-Validation on Full Dataset ---\n",
        "# Combine train and test data for cross-validation\n",
        "X_full = np.vstack([X_train_rfe, X_test_rfe])\n",
        "y_full = np.concatenate([y_train, y_test])\n",
        "\n",
        "# Perform 5-fold cross-validation using the best model\n",
        "cv_scores = cross_val_score(best_model, X_full, y_full, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "print(f\"\\nCross-Validation Results (5-fold):\")\n",
        "print(f\"Mean Accuracy: {cv_scores.mean():.4f}\")\n",
        "print(f\"Standard Deviation: {cv_scores.std():.4f}\")\n",
        "print(f\"Individual Fold Scores: {cv_scores}\")"
      ],
      "metadata": {
        "id": "CoRJS_BxHQSV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b839599e-ad16-42ac-d65a-9f7d0363d4da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial selected features: ['last_purchase_month', 'promotion_end_month', 'product_manufacture_month', 'month_of_year', 'product_expiry_date_year', 'product_manufacture_year', 'transaction_year', 'product_expiry_date_month', 'transaction_month', 'high_value_purchase', 'week_of_year', 'promotion_start_month', 'day_of_week', 'purchase_frequency', 'customer_city', 'gender', 'weekend', 'store_state', 'email_subscriptions', 'store_location', 'high_value_quantity', 'customer_support_calls', 'product_review_count', 'days_since_last_purchase', 'online_purchases', 'distance_to_store', 'product_rating', 'total_transactions', 'product_weight', 'total_items_purchased', 'unit_price', 'total_returned_value', 'membership_years', 'discount_applied', 'avg_discount_used', 'product_shelf_life', 'total_returned_items', 'transaction_hour', 'min_single_purchase_value', 'number_of_children', 'product_stock', 'avg_purchase_value', 'avg_items_per_transaction', 'website_visits', 'age', 'max_single_purchase_value', 'avg_spent_per_category', 'total_discounts_received', 'product_return_rate', 'avg_transaction_value', 'in_store_purchases']\n",
            "After Balancing: Counter({0: 10000, 1: 10000, 2: 10000})\n",
            "Number of Features Selected by RFECV: 36\n",
            "Selected Transformed Features: ['num__customer_support_calls', 'num__product_review_count', 'num__days_since_last_purchase', 'num__online_purchases', 'num__distance_to_store', 'num__product_rating', 'num__total_transactions', 'num__product_weight', 'num__total_items_purchased', 'num__unit_price', 'num__total_returned_value', 'num__membership_years', 'num__discount_applied', 'num__avg_discount_used', 'num__product_shelf_life', 'num__total_returned_items', 'num__transaction_hour', 'num__min_single_purchase_value', 'num__number_of_children', 'num__product_stock', 'num__avg_purchase_value', 'num__avg_items_per_transaction', 'num__website_visits', 'num__age', 'num__max_single_purchase_value', 'num__avg_spent_per_category', 'num__total_discounts_received', 'num__product_return_rate', 'num__avg_transaction_value', 'num__in_store_purchases', 'cat__last_purchase_month_12', 'cat__product_manufacture_month_12', 'cat__product_expiry_date_year_2023', 'cat__product_manufacture_year_2019', 'cat__transaction_year_2021', 'cat__email_subscriptions_Yes']\n",
            "Selected Original Features: ['customer_support_calls', 'product_review_count', 'days_since_last_purchase', 'online_purchases', 'distance_to_store', 'product_rating', 'total_transactions', 'product_weight', 'total_items_purchased', 'unit_price', 'total_returned_value', 'membership_years', 'discount_applied', 'avg_discount_used', 'product_shelf_life', 'total_returned_items', 'transaction_hour', 'min_single_purchase_value', 'number_of_children', 'product_stock', 'avg_purchase_value', 'avg_items_per_transaction', 'website_visits', 'age', 'max_single_purchase_value', 'avg_spent_per_category', 'total_discounts_received', 'product_return_rate', 'avg_transaction_value', 'in_store_purchases']\n",
            "Best Hyperparameters: {'classifier__criterion': 'gini', 'classifier__max_depth': 10, 'classifier__min_samples_leaf': 8, 'classifier__min_samples_split': 2}\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Cluster 0       0.52      0.55      0.54      3007\n",
            "   Cluster 1       0.54      0.53      0.54      2995\n",
            "   Cluster 2       0.55      0.54      0.55      2998\n",
            "\n",
            "    accuracy                           0.54      9000\n",
            "   macro avg       0.54      0.54      0.54      9000\n",
            "weighted avg       0.54      0.54      0.54      9000\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1651  711  645]\n",
            " [ 743 1585  667]\n",
            " [ 756  627 1615]]\n",
            "\n",
            "Accuracy: 0.5390\n",
            "Recall: 0.5390\n",
            "Precision: 0.5394\n",
            "Specificity for Cluster 0: 0.7499\n",
            "Specificity for Cluster 1: 0.7772\n",
            "Specificity for Cluster 2: 0.7814\n",
            "-----------------------------------------------\n",
            "\n",
            "Cross-Validation Results (5-fold):\n",
            "Mean Accuracy: 0.5426\n",
            "Standard Deviation: 0.0036\n",
            "Individual Fold Scores: [0.5425     0.548      0.54416667 0.537      0.54133333]\n"
          ]
        }
      ]
    }
  ]
}